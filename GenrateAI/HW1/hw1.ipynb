{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690c3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 27 15:25:18 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82ddccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
      "Downloading transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.0.0\n",
      "    Uninstalling transformers-5.0.0:\n",
      "      Successfully uninstalled transformers-5.0.0\n",
      "Successfully installed transformers-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f852024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fd34948189423db40ec92ce8f6d537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c9bd9de8914876a7b76601cc715b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7debf294f94c8e82f9aa7b58ef1365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dacb0afbb554f68b3c8cd11e3377e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ade7a7323644c6879a9ac28703ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0460a6229641fda607b0b8f6ac2a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec8df6196b4057a9bcc253ebd73086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64eee5b912754bae80bba6139388a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "好的，用户让我简短介绍一下大语言模型。首先，我需要确定用户的需求是什么。他们可能是在学习AI的基础知识，或者想了解大语言模型的基本概念。用户可能对大语言模型的定义、用途和特点不太清楚，所以需要简明扼要地回答。\n",
      "\n",
      "接下来，我要考虑用户的身份。可能是学生、研究人员，或者对AI感兴趣的人。所以回答要准确且易于理解，避免专业术语过多，同时涵盖关键点，比如定义、结构、应用场景等。\n",
      "\n",
      "然后，我需要确保回答符合简洁的要求。可能需要分点说明，但用户要求简短，所以可能不需要太多分点，保持语言流畅。还要注意术语的解释，比如“大语言模型”指的是大型语言模型，而不是其他类型的模型，这样用户能明确区分。\n",
      "\n",
      "另外，用户可能希望了解大语言模型的优势，比如处理大量文本、生成文本、适应不同任务的能力等。需要提到这些优势，同时指出其应用场景，比如写作、翻译、客服等，这样用户能更好地理解其实际价值。\n",
      "\n",
      "最后，检查是否有遗漏的重要信息，确保回答全面且准确。可能需要确认是否需要补充一些细节，但根据用户要求，保持简短即可。总结起来，回答应涵盖定义、结构、应用场景和优势，用简洁的语言表达。\n",
      "</think>\n",
      "content: 大语言模型（Large Language Model，LLM）是一种通过大量文本数据训练而成的AI模型，能够理解和生成人类语言。它通过深度神经网络学习语言的结构和规律，具备以下特点：  \n",
      "- **处理复杂文本**：能理解多语言、长文本，并生成高质量的输出。  \n",
      "- **多任务能力**：支持多种语言的翻译、文本生成、对话等任务。  \n",
      "- **应用场景广泛**：用于写作、客服、教育、科研等领域，提升效率和智能化水平。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"请简短介绍一下大语言模型.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8853bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语言模型token的token数量: 151643\n"
     ]
    }
   ],
   "source": [
    "print(\"语言模型token的token数量:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4b1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0号token是： atus\n"
     ]
    }
   ],
   "source": [
    "token_id = 1000\n",
    "print(\"0号token是：\", tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abe8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0号token是： !\"#$%&\n"
     ]
    }
   ],
   "source": [
    "print(\"0号token是：\", tokenizer.decode([0,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e78284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108386]\n",
      "[14990]\n"
     ]
    }
   ],
   "source": [
    "text = \"你好\"\n",
    "text1 = \"hello\"\n",
    "token = tokenizer.encode(text)\n",
    "print(token)\n",
    "token1 = tokenizer.encode(text1)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac05948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id: 56940, token str:                                                                                                                                 , token length: 128\n",
      "token id: 89180, token str: //----------------------------------------------------------------------------------------------------------------, token length: 114\n",
      "token id: 66207, token str:  ----------------------------------------------------------------------------------------------------------------, token length: 113\n",
      "token id: 56342, token str: //------------------------------------------------------------------------------------------------, token length: 98\n",
      "token id: 59809, token str:  ------------------------------------------------------------------------------------------------, token length: 97\n",
      "token id: 86544, token str: /************************************************************************************************, token length: 97\n",
      "token id: 60629, token str: ////////////////////////////////////////////////////////////////////////////////////////////////, token length: 96\n",
      "token id: 98474, token str: ------------------------------------------------------------------------------------------------, token length: 96\n",
      "token id: 86766, token str:                                                                                                , token length: 95\n",
      "token id: 79871, token str:                                                                                            , token length: 91\n",
      "token id: 65271, token str:                                                                                        , token length: 87\n",
      "token id: 51475, token str:                                                                                    , token length: 83\n",
      "token id: 50561, token str: //--------------------------------------------------------------------------------, token length: 82\n",
      "token id: 65900, token str:  =================================================================================, token length: 82\n",
      "token id: 87567, token str: //------------------------------------------------------------------------------\n",
      "\n",
      ", token length: 82\n",
      "token id: 93682, token str:  ******************************************************************************/\n",
      "\n",
      ", token length: 82\n",
      "token id: 94303, token str: //================================================================================, token length: 82\n",
      "token id: 98320, token str: ////////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      ", token length: 82\n",
      "token id: 36713, token str: /*******************************************************************************\n",
      ", token length: 81\n",
      "token id: 39373, token str:  --------------------------------------------------------------------------------, token length: 81\n",
      "token id: 45814, token str: //------------------------------------------------------------------------------\n",
      ", token length: 81\n",
      "token id: 58869, token str: ////////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 81\n",
      "token id: 75632, token str: /********************************************************************************, token length: 81\n",
      "token id: 76550, token str:  ********************************************************************************, token length: 81\n",
      "token id: 79371, token str:  ******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 79403, token str: *******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 84999, token str: /******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 90730, token str: ################################################################################\n",
      ", token length: 81\n",
      "token id: 95429, token str:                                                                                  , token length: 81\n",
      "token id: 28226, token str: ////////////////////////////////////////////////////////////////////////////////, token length: 80\n",
      "token id: 40486, token str: ################################################################################, token length: 80\n",
      "token id: 43449, token str: --------------------------------------------------------------------------------, token length: 80\n",
      "token id: 44142, token str: //-----------------------------------------------------------------------------\n",
      ", token length: 80\n",
      "token id: 53196, token str: /******************************************************************************\n",
      ", token length: 80\n",
      "token id: 61693, token str: ********************************************************************************, token length: 80\n",
      "token id: 63394, token str: ================================================================================, token length: 80\n",
      "token id: 76737, token str: ///////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 80\n",
      "token id: 79448, token str: ###############################################################################\n",
      ", token length: 80\n",
      "token id: 85785, token str:  ******************************************************************************\n",
      ", token length: 80\n",
      "token id: 97417, token str:                                                                                 , token length: 80\n",
      "token id: 39484, token str:                                                                                , token length: 79\n",
      "token id: 51814, token str:  -----------------------------------------------------------------------------\n",
      ", token length: 79\n",
      "token id: 68132, token str: /*****************************************************************************\n",
      ", token length: 79\n",
      "token id: 78938, token str: //----------------------------------------------------------------------------\n",
      ", token length: 79\n",
      "token id: 82049, token str:  =============================================================================\n",
      ", token length: 79\n",
      "token id: 94464, token str: //---------------------------------------------------------------------------\n",
      "\n",
      ", token length: 79\n",
      "token id: 18012, token str: //----------------------------------------------------------------------------, token length: 78\n",
      "token id: 57307, token str: //---------------------------------------------------------------------------\n",
      ", token length: 78\n",
      "token id: 57906, token str: /*----------------------------------------------------------------------------, token length: 78\n",
      "token id: 58007, token str: /****************************************************************************\n",
      ", token length: 78\n",
      "token id: 63538, token str:  ----------------------------------------------------------------------------\n",
      ", token length: 78\n",
      "token id: 64226, token str: //****************************************************************************, token length: 78\n",
      "token id: 81221, token str:                                                                               , token length: 78\n",
      "token id: 83894, token str: /////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 78\n",
      "token id: 89318, token str:  ============================================================================\n",
      ", token length: 78\n",
      "token id: 22565, token str: /****************************************************************************, token length: 77\n",
      "token id: 23852, token str:  ----------------------------------------------------------------------------, token length: 77\n",
      "token id: 28644, token str:  ****************************************************************************, token length: 77\n",
      "token id: 58591, token str:                                                                              , token length: 77\n",
      "token id: 60915, token str: #----------------------------------------------------------------------------, token length: 77\n",
      "token id: 61250, token str:  |--------------------------------------------------------------------------\n",
      ", token length: 77\n",
      "token id: 70988, token str: /***************************************************************************\n",
      ", token length: 77\n",
      "token id: 78757, token str:  ---------------------------------------------------------------------------\n",
      ", token length: 77\n",
      "token id: 86200, token str:  ############################################################################, token length: 77\n",
      "token id: 22349, token str: ----------------------------------------------------------------------------, token length: 76\n",
      "token id: 27183, token str: ////////////////////////////////////////////////////////////////////////////, token length: 76\n",
      "token id: 32041, token str: ############################################################################, token length: 76\n",
      "token id: 33518, token str: ****************************************************************************, token length: 76\n",
      "token id: 42080, token str: |--------------------------------------------------------------------------\n",
      ", token length: 76\n",
      "token id: 55799, token str:                                                                             , token length: 76\n",
      "token id: 97942, token str:  --------------------------------------------------------------------------\n",
      ", token length: 76\n",
      "token id: 14642, token str:                                                                            , token length: 75\n",
      "token id: 17535, token str:  **************************************************************************, token length: 75\n",
      "token id: 25885, token str: --------------------------------------------------------------------------\n",
      ", token length: 75\n",
      "token id: 71508, token str:  //////////////////////////////////////////////////////////////////////////, token length: 75\n",
      "token id: 80550, token str:  -------------------------------------------------------------------------\n",
      ", token length: 75\n",
      "token id: 38368, token str:  =========================================================================, token length: 74\n",
      "token id: 41056, token str:                                                                           , token length: 74\n",
      "token id: 65080, token str: //************************************************************************, token length: 74\n",
      "token id: 90915, token str:  -------------------------------------------------------------------------, token length: 74\n",
      "token id: 96581, token str:  ------------------------------------------------------------------------\n",
      ", token length: 74\n",
      "token id: 97005, token str:  /************************************************************************, token length: 74\n",
      "token id: 10534, token str:  ************************************************************************, token length: 73\n",
      "token id: 11370, token str: /************************************************************************, token length: 73\n",
      "token id: 46145, token str:                                                                          , token length: 73\n",
      "token id: 93858, token str:  ########################################################################, token length: 73\n",
      "token id: 5596, token str: ************************************************************************, token length: 72\n",
      "token id: 35109, token str: ////////////////////////////////////////////////////////////////////////, token length: 72\n",
      "token id: 37144, token str:                                                                         , token length: 72\n",
      "token id: 69061, token str: ########################################################################, token length: 72\n",
      "token id: 89694, token str:  ----------------------------------------------------------------------\n",
      ", token length: 72\n",
      "token id: 22335, token str:                                                                        , token length: 71\n",
      "token id: 62117, token str: ----------------------------------------------------------------------\n",
      ", token length: 71\n",
      "token id: 95180, token str:  //////////////////////////////////////////////////////////////////////, token length: 71\n",
      "token id: 46771, token str:                                                                       , token length: 70\n",
      "token id: 53264, token str: ----------------------------------------------------------------------, token length: 70\n",
      "token id: 42708, token str:                                                                      , token length: 69\n",
      "token id: 43200, token str: ////////////////////////////////////////////////////////////////////, token length: 68\n",
      "token id: 51068, token str:                                                                     , token length: 68\n",
      "token id: 19273, token str:                                                                    , token length: 67\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "\n",
    "for token_id in range(tokenizer.vocab_size):\n",
    "    str = tokenizer.decode(token_id)\n",
    "    token_list.append((token_id, str, len(str)))\n",
    "\n",
    "token_list.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "k = 100\n",
    "for i in range(k):\n",
    "    token_id, token_str, token_length = token_list[i]\n",
    "    print(f\"token id: {token_id}, token str: {token_str}, token length: {token_length}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed5a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户的输入是: 在二进制中，1+1=10\n",
      "模型的原始输入： tensor([[18493, 40820, 41299, 43316, 15946,  3837,    16,    10,    16,    28,\n",
      "            16,    15]])\n",
      "模型的原始输出的shape: torch.Size([1, 12, 151936])\n",
      "几率最高的前10个token是:\n",
      "token_id: 3837, token_str: ，, token_prob: 0.5742\n",
      "token_id: 1773, token_str: 。, token_prob: 0.0996\n",
      "token_id: 11, token_str: ,, token_prob: 0.0500\n",
      "token_id: 9909, token_str: （, token_prob: 0.0471\n",
      "token_id: 9370, token_str: 的, token_prob: 0.0391\n",
      "token_id: 33108, token_str: 和, token_prob: 0.0237\n",
      "token_id: 51463, token_str: 表示, token_prob: 0.0184\n",
      "token_id: 96050, token_str: ，在, token_prob: 0.0098\n",
      "token_id: 15, token_str: 0, token_prob: 0.0093\n",
      "token_id: 24968, token_str: ；, token_prob: 0.0082\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompt = \"在二进制中，1+1=10\"\n",
    "print(\"用户的输入是:\", prompt)\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"模型的原始输入：\", input_ids)\n",
    "\n",
    "outputs = model(input_ids.to(model.device))\n",
    "print(\"模型的原始输出的shape:\", outputs.logits.shape)\n",
    "\n",
    "last_logits = outputs.logits[:,-1,:]\n",
    "probabilities = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "top_k = 10\n",
    "top_p, top_indices = torch.topk(probabilities, top_k)\n",
    "print(f\"几率最高的前{top_k}个token是:\")\n",
    "for i in range(top_k):\n",
    "    token_id = top_indices[0][i].item()\n",
    "    token_prob = top_p[0][i].item()\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    print(f\"token_id: {token_id}, token_str: {token_str}, token_prob: {token_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26d9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在的promt是: 北京大学是中国\n",
      "下一个token是： 的\n",
      "现在的promt是: 北京大学是中国的\n",
      "下一个token是： 著名\n",
      "现在的promt是: 北京大学是中国的著名\n",
      "下一个token是： 学\n",
      "现在的promt是: 北京大学是中国的著名学\n",
      "下一个token是： 府\n",
      "现在的promt是: 北京大学是中国的著名学府\n",
      "下一个token是： ，\n",
      "现在的promt是: 北京大学是中国的著名学府，\n",
      "下一个token是： 拥有\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有\n",
      "下一个token是： 众多\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多\n",
      "下一个token是： 优秀的\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的\n",
      "下一个token是： 学者\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者\n",
      "下一个token是： 和\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和\n",
      "下一个token是： 科研\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和科研\n",
      "下一个token是： 人员\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和科研人员\n",
      "下一个token是： 。\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和科研人员。\n",
      "下一个token是： 在\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和科研人员。在\n",
      "下一个token是： 学术\n",
      "现在的promt是: 北京大学是中国的著名学府，拥有众多优秀的学者和科研人员。在学术\n",
      "下一个token是： 研究\n"
     ]
    }
   ],
   "source": [
    "prompt = \"北京大学是中国\"\n",
    "length = 16\n",
    "\n",
    "for i in range(length):\n",
    "    print(\"现在的promt是:\", prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids.to(model.device))\n",
    "    last_logits = outputs.logits[:,-1,:]\n",
    "    probabilities = torch.softmax(last_logits, dim=-1)\n",
    "    top_p, top_indices = torch.topk(probabilities, 1)\n",
    "    token_id = top_indices[0][0].item()\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    print(\"下一个token是：\", token_str)\n",
    "    prompt += token_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc4892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在的promt是: 北京大学是中国\n",
      "下一个token是： 的\n",
      "现在的promt是: 北京大学是中国的\n",
      "下一个token是： 首都\n",
      "现在的promt是: 北京大学是中国的首都\n",
      "下一个token是： ，\n",
      "现在的promt是: 北京大学是中国的首都，\n",
      "下一个token是： 为\n",
      "现在的promt是: 北京大学是中国的首都，为\n",
      "下一个token是： 最初的\n",
      "现在的promt是: 北京大学是中国的首都，为最初的\n",
      "下一个token是： **\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**\n",
      "下一个token是： 中国\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国\n",
      "下一个token是： 文字\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字\n",
      "下一个token是： **\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字**\n",
      "下一个token是：  rune\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune\n",
      "下一个token是：  code\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code\n",
      "下一个token是： 。\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。\n",
      "下一个token是： 从\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从\n",
      "下一个token是： 神秘\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘\n",
      "下一个token是： 陆\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆\n",
      "下一个token是： 地\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地\n",
      "下一个token是： 文明\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明\n",
      "下一个token是： 的\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的\n",
      "下一个token是： 起源\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源\n",
      "下一个token是： ，\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，\n",
      "下一个token是： 到\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到\n",
      "下一个token是： 最终\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终\n",
      "下一个token是： 成为一个\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个\n",
      "下一个token是： 国际\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际\n",
      "下一个token是： 性的\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的\n",
      "下一个token是： 古代\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代\n",
      "下一个token是： 遗址\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代遗址\n",
      "下一个token是： ，\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代遗址，\n",
      "下一个token是： 再到\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代遗址，再到\n",
      "下一个token是： 现在的\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代遗址，再到现在的\n",
      "下一个token是： 高等\n",
      "现在的promt是: 北京大学是中国的首都，为最初的**中国文字** rune code。从神秘陆地文明的起源，到最终成为一个国际性的古代遗址，再到现在的高等\n",
      "下一个token是： 学\n"
     ]
    }
   ],
   "source": [
    "#掷骰子，完全按几率掷，导致语句混乱\n",
    "prompt = \"北京大学是中国\"\n",
    "length = 32\n",
    "\n",
    "for i in range(length):\n",
    "    print(\"现在的promt是:\", prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids.to(model.device))\n",
    "    last_logits = outputs.logits[:,-1,:]\n",
    "    probabilities = torch.softmax(last_logits, dim=-1)\n",
    "    token_id = torch.multinomial(probabilities, num_samples=1).squeeze()\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    print(\"下一个token是：\", token_str)\n",
    "    prompt += token_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd86e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
