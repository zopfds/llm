{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690c3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 26 13:35:55 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82ddccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
      "Downloading transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.0.0\n",
      "    Uninstalling transformers-5.0.0:\n",
      "      Successfully uninstalled transformers-5.0.0\n",
      "Successfully installed transformers-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f852024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca4c084313c4b0a961e967beaacd6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b447c20fc17e4a3099d7ec8ecb8f9610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f32fd8822b43bd8a6d43c0c87471fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8e01220d0b423386861f5f4e9c336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc37a740eca40259d6c26e6e8d5a44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc30975a40d4f428d5dfa9c54914a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8660a4207e704b9dbc1fb57de159ddd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e1014252c346b39883c280e41ff388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "好的，用户让我简短介绍一下大语言模型。首先，我需要确定用户的需求是什么。他们可能对大语言模型的基本概念感兴趣，或者想了解它的应用场景。用户可能没有太多背景，所以需要保持回答简洁明了。\n",
      "\n",
      "接下来，我要考虑用户可能的深层需求。他们可能想知道大语言模型的定义、主要特点，以及它们的应用领域。可能还需要提到一些关键点，比如训练数据、计算资源、应用场景等，但要确保信息准确且简短。\n",
      "\n",
      "然后，我需要检查是否有遗漏的信息。比如，是否应该提到模型的架构或训练方法？不过用户要求简短，可能不需要深入。同时，要避免使用专业术语过多，保持口语化。\n",
      "\n",
      "最后，确保回答结构清晰，先定义，再列出关键点，最后总结应用。这样用户能轻松理解。同时，注意用词准确，比如“大语言模型”是专有名词，要正确使用。检查是否有重复或冗余的信息，保持回答简洁。\n",
      "</think>\n",
      "content: 大语言模型是一种基于深度学习的AI系统，能够理解并生成人类语言。它通过大量文本数据训练，具备理解、生成和交互能力，广泛应用于自然语言处理、内容创作、智能助手等领域。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"请简短介绍一下大语言模型.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8853bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语言模型token的token数量: 151643\n"
     ]
    }
   ],
   "source": [
    "print(\"语言模型token的token数量:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4b1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0号token是： atus\n"
     ]
    }
   ],
   "source": [
    "token_id = 1000\n",
    "print(\"0号token是：\", tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9abe8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0号token是： !\"#$%&\n"
     ]
    }
   ],
   "source": [
    "print(\"0号token是：\", tokenizer.decode([0,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07e78284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108386]\n",
      "[14990]\n"
     ]
    }
   ],
   "source": [
    "text = \"你好\"\n",
    "text1 = \"hello\"\n",
    "token = tokenizer.encode(text)\n",
    "print(token)\n",
    "token1 = tokenizer.encode(text1)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eac05948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id: 56940, token str:                                                                                                                                 , token length: 128\n",
      "token id: 89180, token str: //----------------------------------------------------------------------------------------------------------------, token length: 114\n",
      "token id: 66207, token str:  ----------------------------------------------------------------------------------------------------------------, token length: 113\n",
      "token id: 56342, token str: //------------------------------------------------------------------------------------------------, token length: 98\n",
      "token id: 59809, token str:  ------------------------------------------------------------------------------------------------, token length: 97\n",
      "token id: 86544, token str: /************************************************************************************************, token length: 97\n",
      "token id: 60629, token str: ////////////////////////////////////////////////////////////////////////////////////////////////, token length: 96\n",
      "token id: 98474, token str: ------------------------------------------------------------------------------------------------, token length: 96\n",
      "token id: 86766, token str:                                                                                                , token length: 95\n",
      "token id: 79871, token str:                                                                                            , token length: 91\n",
      "token id: 65271, token str:                                                                                        , token length: 87\n",
      "token id: 51475, token str:                                                                                    , token length: 83\n",
      "token id: 50561, token str: //--------------------------------------------------------------------------------, token length: 82\n",
      "token id: 65900, token str:  =================================================================================, token length: 82\n",
      "token id: 87567, token str: //------------------------------------------------------------------------------\n",
      "\n",
      ", token length: 82\n",
      "token id: 93682, token str:  ******************************************************************************/\n",
      "\n",
      ", token length: 82\n",
      "token id: 94303, token str: //================================================================================, token length: 82\n",
      "token id: 98320, token str: ////////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      ", token length: 82\n",
      "token id: 36713, token str: /*******************************************************************************\n",
      ", token length: 81\n",
      "token id: 39373, token str:  --------------------------------------------------------------------------------, token length: 81\n",
      "token id: 45814, token str: //------------------------------------------------------------------------------\n",
      ", token length: 81\n",
      "token id: 58869, token str: ////////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 81\n",
      "token id: 75632, token str: /********************************************************************************, token length: 81\n",
      "token id: 76550, token str:  ********************************************************************************, token length: 81\n",
      "token id: 79371, token str:  ******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 79403, token str: *******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 84999, token str: /******************************************************************************/\n",
      ", token length: 81\n",
      "token id: 90730, token str: ################################################################################\n",
      ", token length: 81\n",
      "token id: 95429, token str:                                                                                  , token length: 81\n",
      "token id: 28226, token str: ////////////////////////////////////////////////////////////////////////////////, token length: 80\n",
      "token id: 40486, token str: ################################################################################, token length: 80\n",
      "token id: 43449, token str: --------------------------------------------------------------------------------, token length: 80\n",
      "token id: 44142, token str: //-----------------------------------------------------------------------------\n",
      ", token length: 80\n",
      "token id: 53196, token str: /******************************************************************************\n",
      ", token length: 80\n",
      "token id: 61693, token str: ********************************************************************************, token length: 80\n",
      "token id: 63394, token str: ================================================================================, token length: 80\n",
      "token id: 76737, token str: ///////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 80\n",
      "token id: 79448, token str: ###############################################################################\n",
      ", token length: 80\n",
      "token id: 85785, token str:  ******************************************************************************\n",
      ", token length: 80\n",
      "token id: 97417, token str:                                                                                 , token length: 80\n",
      "token id: 39484, token str:                                                                                , token length: 79\n",
      "token id: 51814, token str:  -----------------------------------------------------------------------------\n",
      ", token length: 79\n",
      "token id: 68132, token str: /*****************************************************************************\n",
      ", token length: 79\n",
      "token id: 78938, token str: //----------------------------------------------------------------------------\n",
      ", token length: 79\n",
      "token id: 82049, token str:  =============================================================================\n",
      ", token length: 79\n",
      "token id: 94464, token str: //---------------------------------------------------------------------------\n",
      "\n",
      ", token length: 79\n",
      "token id: 18012, token str: //----------------------------------------------------------------------------, token length: 78\n",
      "token id: 57307, token str: //---------------------------------------------------------------------------\n",
      ", token length: 78\n",
      "token id: 57906, token str: /*----------------------------------------------------------------------------, token length: 78\n",
      "token id: 58007, token str: /****************************************************************************\n",
      ", token length: 78\n",
      "token id: 63538, token str:  ----------------------------------------------------------------------------\n",
      ", token length: 78\n",
      "token id: 64226, token str: //****************************************************************************, token length: 78\n",
      "token id: 81221, token str:                                                                               , token length: 78\n",
      "token id: 83894, token str: /////////////////////////////////////////////////////////////////////////////\n",
      ", token length: 78\n",
      "token id: 89318, token str:  ============================================================================\n",
      ", token length: 78\n",
      "token id: 22565, token str: /****************************************************************************, token length: 77\n",
      "token id: 23852, token str:  ----------------------------------------------------------------------------, token length: 77\n",
      "token id: 28644, token str:  ****************************************************************************, token length: 77\n",
      "token id: 58591, token str:                                                                              , token length: 77\n",
      "token id: 60915, token str: #----------------------------------------------------------------------------, token length: 77\n",
      "token id: 61250, token str:  |--------------------------------------------------------------------------\n",
      ", token length: 77\n",
      "token id: 70988, token str: /***************************************************************************\n",
      ", token length: 77\n",
      "token id: 78757, token str:  ---------------------------------------------------------------------------\n",
      ", token length: 77\n",
      "token id: 86200, token str:  ############################################################################, token length: 77\n",
      "token id: 22349, token str: ----------------------------------------------------------------------------, token length: 76\n",
      "token id: 27183, token str: ////////////////////////////////////////////////////////////////////////////, token length: 76\n",
      "token id: 32041, token str: ############################################################################, token length: 76\n",
      "token id: 33518, token str: ****************************************************************************, token length: 76\n",
      "token id: 42080, token str: |--------------------------------------------------------------------------\n",
      ", token length: 76\n",
      "token id: 55799, token str:                                                                             , token length: 76\n",
      "token id: 97942, token str:  --------------------------------------------------------------------------\n",
      ", token length: 76\n",
      "token id: 14642, token str:                                                                            , token length: 75\n",
      "token id: 17535, token str:  **************************************************************************, token length: 75\n",
      "token id: 25885, token str: --------------------------------------------------------------------------\n",
      ", token length: 75\n",
      "token id: 71508, token str:  //////////////////////////////////////////////////////////////////////////, token length: 75\n",
      "token id: 80550, token str:  -------------------------------------------------------------------------\n",
      ", token length: 75\n",
      "token id: 38368, token str:  =========================================================================, token length: 74\n",
      "token id: 41056, token str:                                                                           , token length: 74\n",
      "token id: 65080, token str: //************************************************************************, token length: 74\n",
      "token id: 90915, token str:  -------------------------------------------------------------------------, token length: 74\n",
      "token id: 96581, token str:  ------------------------------------------------------------------------\n",
      ", token length: 74\n",
      "token id: 97005, token str:  /************************************************************************, token length: 74\n",
      "token id: 10534, token str:  ************************************************************************, token length: 73\n",
      "token id: 11370, token str: /************************************************************************, token length: 73\n",
      "token id: 46145, token str:                                                                          , token length: 73\n",
      "token id: 93858, token str:  ########################################################################, token length: 73\n",
      "token id: 5596, token str: ************************************************************************, token length: 72\n",
      "token id: 35109, token str: ////////////////////////////////////////////////////////////////////////, token length: 72\n",
      "token id: 37144, token str:                                                                         , token length: 72\n",
      "token id: 69061, token str: ########################################################################, token length: 72\n",
      "token id: 89694, token str:  ----------------------------------------------------------------------\n",
      ", token length: 72\n",
      "token id: 22335, token str:                                                                        , token length: 71\n",
      "token id: 62117, token str: ----------------------------------------------------------------------\n",
      ", token length: 71\n",
      "token id: 95180, token str:  //////////////////////////////////////////////////////////////////////, token length: 71\n",
      "token id: 46771, token str:                                                                       , token length: 70\n",
      "token id: 53264, token str: ----------------------------------------------------------------------, token length: 70\n",
      "token id: 42708, token str:                                                                      , token length: 69\n",
      "token id: 43200, token str: ////////////////////////////////////////////////////////////////////, token length: 68\n",
      "token id: 51068, token str:                                                                     , token length: 68\n",
      "token id: 19273, token str:                                                                    , token length: 67\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "\n",
    "for token_id in range(tokenizer.vocab_size):\n",
    "    str = tokenizer.decode(token_id)\n",
    "    token_list.append((token_id, str, len(str)))\n",
    "\n",
    "token_list.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "k = 100\n",
    "for i in range(k):\n",
    "    token_id, token_str, token_length = token_list[i]\n",
    "    print(f\"token id: {token_id}, token str: {token_str}, token length: {token_length}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户的输入是: 在二进制中，1+1=10\n",
      "模型的原始输入： tensor([[18493, 40820, 41299, 43316, 15946,  3837,    16,    10,    16,    28,\n",
      "            16,    15]])\n",
      "模型的原始输出的shape: torch.Size([1, 151936])\n",
      "几率最高的前10个token是:\n",
      "token_id: 3837, token_str: ，, token_prob: 0.5742\n",
      "token_id: 1773, token_str: 。, token_prob: 0.0996\n",
      "token_id: 11, token_str: ,, token_prob: 0.0500\n",
      "token_id: 9909, token_str: （, token_prob: 0.0471\n",
      "token_id: 9370, token_str: 的, token_prob: 0.0391\n",
      "token_id: 33108, token_str: 和, token_prob: 0.0237\n",
      "token_id: 51463, token_str: 表示, token_prob: 0.0184\n",
      "token_id: 96050, token_str: ，在, token_prob: 0.0098\n",
      "token_id: 15, token_str: 0, token_prob: 0.0093\n",
      "token_id: 24968, token_str: ；, token_prob: 0.0082\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompt = \"在二进制中，1+1=10\"\n",
    "print(\"用户的输入是:\", prompt)\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"模型的原始输入：\", input_ids)\n",
    "\n",
    "outputs = model(input_ids.to(model.device))\n",
    "print(\"模型的原始输出的shape:\", outputs.shape)\n",
    "\n",
    "last_logits = outputs.logits[:,-1,:]\n",
    "probabilities = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "top_k = 10\n",
    "top_p, top_indices = torch.topk(probabilities, top_k)\n",
    "print(f\"几率最高的前{top_k}个token是:\")\n",
    "for i in range(top_k):\n",
    "    token_id = top_indices[0][i].item()\n",
    "    token_prob = top_p[0][i].item()\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    print(f\"token_id: {token_id}, token_str: {token_str}, token_prob: {token_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d9119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
